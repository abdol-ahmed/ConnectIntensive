# ConnectIntensive
Jupyter Notebooks for the Connect Intensive MLND Program
<<<<<<< HEAD
Add my own comments
=======

# Lesson Notebooks
  - `lesson-00.ipynb` : Hello Jupyter Notebook!
    - A "hello world" notebook to introduce the Jupyter IDE
    - Introduces import statements for commonly-used modules and packages
  - `lesson-01.ipynb` : An intro to Statistical Analysis using `pandas`
    - Introduces the `Series` and `DataFrame` objects in `pandas`
    - Defines categorical variables
    - Covers basic descriptive statistics: mean, median, min/max
    - Label-based `.loc` and index-based location `.iloc`
    - Boolean indexing, how to slice a `DataFrame`
    - Exercises in exploratory data analysis, emphasizing `groupby` and `plot`
  - `lesson-02.ipynb` : Working with the Enron Data Set
    - Covers the `pickle` module for saving objects
    - Magic functions in Jupyter notebooks
    - Use of the `stack` and `unstack` functions
    - Exercises in exploratory data analysis on the Enron data set
  - `lesson-03-part-01.ipynb` : Building and Evaluating Models with `sklearn`
    - Perform exploratory data analysis on a dataset
    - Tidy a data set so that it will be compatible with the `sklearn` library
  - `lesson-03-part-02.ipynb` : Building and Evaluating Models with `sklearn`
    - Make decision tree classifiers on the tidied dataset from part 01
    - Compute the accuracy of a model on both the training and validation (testing) data
    - Adjust hyperparameters to see the effects on model accuracy
    - Visualize decision trees and introduce the Gini impurity
  - `lesson-04-part-01.ipynb` : Bayes NLP Mini-Project
    - Understand how Bayes Rule derives from conditional probability
    - Write methods, applying Bayesian learning, to simple word-prediction tasks
    - Practice with `str.split()` and python dictionaries
  - `lesson-05.ipynb` : Classification with Support Vector Machines
    - Introduces additional plotting functionality in `matplotlib.pyplot`
      - Boxplots for depicting interquartile range (IQR), median, max, min, outliers
      - Scatterplots for 2-D representation of two features.
    - Introduction to Support Vector Machines in `sklearn`
      - An introduction to kernels
      - Hard-margin versus soft-margin SVMs
      - Overview of `SVC` hyperparameters: `C`, `gamma`, `degree`, etc.
    - Visualize decision boundaries resulting from the different kernels
    - Practice with `GridSearchCV`
<<<<<<< .merge_file_JfBjxV
>>>>>>> 4ba96f7d380c209a527739228633dbe80f27f5a8
=======
  - `lesson-06-part-01.ipynb` : Clustering Mini-Project
    - Perform [k-means clustering](http://scikit-learn.org/stable/modules/clustering.html#k-means) on the Enron Data Set.
    - Visualize different clusters that form before and after feature scaling.
    - Plot decision boundaries that arise from k-means clustering using two features.
  - `lesson-06-part-02.ipynb` : PCA Mini-Project
    - Perform Principal Component Analysis (PCA) on a large set of features.
    - Recognize differences between `train_test_split()` and `StratifiedShuffleSplit()`.
    - Introduce the `class_weight` parameter for `SVC()`.
    - Visualize the eigenfaces (orthonormal basis of components) that result from PCA.
  
    
>>>>>>> .merge_file_kyaxcJ
