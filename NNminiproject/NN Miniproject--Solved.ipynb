{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests completed successfully\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "# \n",
    "# In this exercise, you will put the finishing touches on a perceptron class.\n",
    "#\n",
    "# Finish writing the activate() method by using np.dot to compute signal\n",
    "# strength and then add in a threshold for perceptron activation.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Perceptron0(object):\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def activate(self,inputs):\n",
    "        \"\"\"\n",
    "        Takes in @param inputs, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\" \n",
    "\n",
    "        # First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(inputs,self.weights)\n",
    "        \n",
    "        # Then return 0 or 1 depending on strength compared to threshold  \n",
    "        \n",
    "        # return int(strength > self.threshold) # Fancy way casting boolean variables into int\n",
    "        if strength > self.threshold:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    p1 = Perceptron0(np.array([1, 2]), 0.)\n",
    "    assert p1.activate(np.array([ 1,-1])) == 0 # < threshold --> 0\n",
    "    assert p1.activate(np.array([-1, 1])) == 1 # > threshold --> 1\n",
    "    assert p1.activate(np.array([ 2,-1])) == 0 # on threshold --> 0\n",
    "    return True\n",
    "\n",
    "if test():\n",
    "    print \"All tests completed successfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "What are the advantages of using some threshold and step function rather than just outputting the weighted inputs (dot product)?\n",
    "### Answer(s)\n",
    "There's not really one right answer here. One thought is that it creates discrete outputs needed for classification. It also creates a tunable hyperparameter that can control the sensitivity of the perceptron unit. More mathmatically, the threshold moves the position of the decision boundry. I'm sure there are other answers as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What _parameter_ is learnable in a perceptron? In other words, what can be modified to allow the perceptron or a network of perceptrons to model an arbitrary function. \n",
    "### Answer\n",
    "Weights. We could modify the thresholds, but we can also include threshold changes in the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What does the input to a network of perceptrons look like?\n",
    "A) Tensor network of weights\n",
    "B) Matrix of numerical values\n",
    "C) Matrix of classifcations \n",
    "D) Matrix of numerical values and classifications for each row\n",
    "### Answer\n",
    "D) Matrix of numerical values and classifications for each row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Are Neural Networks used for classification or regression?\n",
    "### Answer\n",
    "Trick question, NN can be used for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests completed sucessfully\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will update the perceptron class so that it can update\n",
    "# its weights.\n",
    "#\n",
    "# Finish writing the update() method so that it updates the weights according\n",
    "# to the perceptron update rule. Updates should be performed online, revising\n",
    "# the weights after each data point.\n",
    "# \n",
    "# ----------\n",
    "\n",
    "\n",
    "class Perceptron1(Perceptron0):\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        super(Perceptron1,self).__init__(*args )\n",
    "\n",
    "\n",
    "    def update(self, X, y, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param X consisting of a LIST of inputs and a\n",
    "        1D array @param y, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to the perceptron training\n",
    "        rule using these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        for i, x_i in enumerate(X):\n",
    "            y_hat = self.activate(x_i)\n",
    "            delta_w = (eta * (y[i] - y_hat)) * x_i\n",
    "            self.weights = self.weights + delta_w\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-6):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    p1 = Perceptron1(np.array([1,1,1]),0)\n",
    "    p1.update(np.array([[2,0,-3]]), np.array([1]))\n",
    "    assert sum_almost_equal(p1.weights, np.array([1.2, 1, 0.7]))\n",
    "\n",
    "    p2 = Perceptron1(np.array([1,2,3]),0)\n",
    "    p2.update(np.array([[3,2,1],[4,0,-1]]),np.array([0,0]))\n",
    "    assert sum_almost_equal(p2.weights, np.array([0.7, 1.8, 2.9]))\n",
    "\n",
    "    p3 = Perceptron1(np.array([3,0,2]),0)\n",
    "    p3.update(np.array([[2,-2,4],[-1,-3,2],[0,2,1]]),np.array([0,1,0]))\n",
    "    assert sum_almost_equal(p3.weights, np.array([2.7, -0.3, 1.7]))\n",
    "    return True\n",
    "if test():\n",
    "    print \"All tests completed sucessfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layered Network\n",
    "Layered networks consist of some input layer, some number hidden nodes, and some output layer that outputs the classifcation or regression results. \n",
    "\n",
    "Given the network structure below with weights on the edges of the graph. What will be the output of this network.\n",
    "\n",
    "![](Network1.png \"NN\")\n",
    "\n",
    "```\n",
    "[\n",
    "[ [1], [2], [3] ]     # Input layer (these are not weights, but input values)\n",
    "[[1,1,-5],[3,-4,2] ]  # Hidden layer\n",
    "[ [2,-1] ]            # Output layer\n",
    "]\n",
    "```\n",
    "\n",
    "## Answer\n",
    "Inputs * hidden layer weights\n",
    "$$ \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}  \\begin{pmatrix} 1  & 3 \\\\ 1  & -4 \\\\ -5 & 2 \\end{pmatrix} = \\begin{pmatrix} -12 & 1 \\end{pmatrix} $$\n",
    "\n",
    "hidden layer outputs * output layer weights = final output\n",
    "$$ \\begin{pmatrix} -12 & 1 \\end{pmatrix}  \\begin{pmatrix} 2 \\\\ -1  \\end{pmatrix} = -25 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs [0, 0]\n",
      "AND activation 0\n",
      "Inputs plus output from AND, [0, 0, 0]\n",
      "Output Value: 0\n",
      "\n",
      "\n",
      "\n",
      "Inputs [0, 1]\n",
      "AND activation 0\n",
      "Inputs plus output from AND, [0, 1, 0]\n",
      "Output Value: 1\n",
      "\n",
      "\n",
      "\n",
      "Inputs [1, 0]\n",
      "AND activation 0\n",
      "Inputs plus output from AND, [1, 0, 0]\n",
      "Output Value: 1\n",
      "\n",
      "\n",
      "\n",
      "Inputs [1, 1]\n",
      "AND activation 1\n",
      "Inputs plus output from AND, [1, 1, 1]\n",
      "Output Value: 0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will create a network of perceptrons that can represent\n",
    "# the XOR function, using a network structure like those shown in the previous\n",
    "# quizzes.\n",
    "#\n",
    "# You will need to do two things:\n",
    "# First, create a network of perceptrons with the correct weights\n",
    "# Second, define a procedure EvalNetwork() which takes in a list of inputs and\n",
    "# outputs the value of this network.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "            \n",
    "Network = [\n",
    "    # input layer, declare input layer perceptrons here\n",
    "    [ Perceptron0(np.array([1,1]),1)], \\\n",
    "    # output node, declare output layer perceptron here\n",
    "    [ Perceptron0(np.array([2,2,-4]),1) ]\n",
    "]\n",
    "\n",
    "# Part 2: Define a procedure to compute the output of the network, given inputs\n",
    "def EvalNetwork(inputValues, Network):\n",
    "    \n",
    "    # Be sure your output values are single numbers\n",
    "    \n",
    "    #return OutputValues\n",
    "    inputValues = inputValues.tolist()\n",
    "    print \"Inputs\", inputValues\n",
    "    print \"AND activation\", Network[0][0].activate(inputValues)\n",
    "    inputValues.append(Network[0][0].activate(inputValues))\n",
    "    print \"Inputs plus output from AND,\", inputValues\n",
    "    OutputValue = Network[1][0].activate(inputValues)\n",
    "    print \"Output Value:\", OutputValue\n",
    "    print '\\n\\n'\n",
    "    return OutputValue\n",
    "            \n",
    "    # Be sure your output value is a single number\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    \"\"\"\n",
    "    EvalNetwork(np.array([0,0]), Network) # 0 XOR 0 = 0\n",
    "    EvalNetwork(np.array([0,1]), Network) # 0 XOR 1 = 1\n",
    "    EvalNetwork(np.array([1,0]), Network) # 1 XOR 0 = 1\n",
    "    EvalNetwork(np.array([1,1]), Network) # 1 XOR 1 = 0\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Discretion\n",
    "\n",
    "The outputs of perceptron units are discrete. Consider a network with the structure [2,2,1], that is 2 input nodes, two hidden nodes, and 1 output node. How many possible outputs to this network are there? _Hint: The answer is NOT two_\n",
    "\n",
    "### Answer\n",
    "4 - The output of the last node is a combination of the previous two hidden nodes, each of which can take on two possible values. Note that the output is not binary because each hidden node can take output any two numerical values (since they are combined with weights). Thus, there is a maximum of 4 differernt values that can be sent to the final node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def activate(strength):\n",
    "    # Try out different functions here. Input strength will be a number, with\n",
    "    # another number as output.\n",
    "    return np.power(strength,2)\n",
    "    return 1/ (1 + np.exp(-strength)) # sigmoid\n",
    "    return 2 / (1 + np.exp(-2*3)) -1  # tanh\n",
    "    return 0 if strength < 0 else strength # ReLu\n",
    "\n",
    "    \n",
    "def activation_derivative(activate, strength):\n",
    "    #numerically approximate\n",
    "    return (activate(strength+1e-5)-activate(strength-1e-5))/(2e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests completed sucessfully\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "# \n",
    "# As with the previous perceptron exercises, you will complete some of the core\n",
    "# methods of a sigmoid unit class.\n",
    "#\n",
    "# There are two functions for you to finish:\n",
    "# First, in activate(), write the sigmoid activation function.\n",
    "# Second, in update(), write the gradient descent update rule. Updates should be\n",
    "#   performed online, revising the weights after each data point.\n",
    "# \n",
    "# ----------\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with sigmoid activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1])):\n",
    "        \"\"\"\n",
    "        Initialize weights based on input arguments. Note that no type-checking\n",
    "        is being performed here for simplicity of code.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "\n",
    "        # NOTE: You do not need to worry about these two attribues for this\n",
    "        # programming quiz, but these will be useful for if you want to create\n",
    "        # a network out of these sigmoid units!\n",
    "        self.last_input = 0 # strength of last input\n",
    "        self.delta      = 0 # error signal\n",
    "    \n",
    "    def logistic(self,x):\n",
    "        return 1/ (1 + np.exp(-x))\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a sigmoid unit with given inputs based on unit\n",
    "        weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # First calculate the strength of the input signal.\n",
    "        strength = np.dot(values, self.weights)\n",
    "        self.last_input = strength\n",
    "        \n",
    "        # TODO: Modify strength using the sigmoid activation function and\n",
    "        # return as output signal.\n",
    "        # HINT: You may want to create a helper function to compute the\n",
    "        #   logistic function since you will need it for the update function.\n",
    "        \n",
    "        return self.logistic(strength)\n",
    "    \n",
    "    def update(self, values, train, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to gradient descent using\n",
    "        these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: for each data point...\n",
    "        for x_i, y_true in zip(values, train):\n",
    "            # obtain the output signal for that point\n",
    "            y_pred = self.activate(x_i)\n",
    "\n",
    "            # YOUR CODE HERE\n",
    "            ddx = y_pred * (1 - y_pred)\n",
    "            delta_w = eta *(y_true - y_pred) * ddx * x_i\n",
    "            self.weights = self.weights + delta_w\n",
    "\n",
    "            # TODO: update self.weights based on learning rate, signal accuracy,\n",
    "            # function slope (derivative) and input value\n",
    "            \n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-5):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    u1 = Sigmoid(weights=[3,-2,1])\n",
    "    assert abs(u1.activate(np.array([1,2,3])) - 0.880797) < 1e-5\n",
    "    \n",
    "    u1.update(np.array([[1,2,3]]),np.array([0]))\n",
    "    assert sum_almost_equal(u1.weights, np.array([2.990752, -2.018496, 0.972257]))\n",
    "\n",
    "    u2 = Sigmoid(weights=[0,3,-1])\n",
    "    u2.update(np.array([[-3,-1,2],[2,1,2]]),np.array([1,0]))\n",
    "    assert sum_almost_equal(u2.weights, np.array([-0.030739, 2.984961, -1.027437]))\n",
    "    return True\n",
    "if  test():\n",
    "    print \"All tests completed sucessfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
