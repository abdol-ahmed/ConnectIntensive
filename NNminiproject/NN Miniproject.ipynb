{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Neural Networks\n",
    "\n",
    "Recall that Neural Networks are layered networks of nodes. One of the most simple nodes is a called the perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptrons\n",
    "Perceptrons are not new to machine learning. In fact, here is an image of a machine designed by the US Navy for image classification.  \n",
    "![made of wires](Mark_I_perceptron.jpeg \"Mark I perceptron Machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most simply a perceptron maps a series of inputs to an output. Each input is weighted by a cetrain amount before some mapping is applied to the inputs that determines the output of the perceptron. We will compute the weighting of the inputs by taking the dot product of the input vector and the weight vector. This number is also known as the strenth or the activity of the inputs. Our mapping to the perceptron output will be a simple step function that takes in the strenth of the inputs and compares the strength to some predefined threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests completed\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "# \n",
    "# In this exercise, you will put the finishing touches on a perceptron class.\n",
    "#\n",
    "# Finish writing the activate() method by using np.dot to compute signal\n",
    "# strength and then add in a threshold for perceptron activation.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Perceptron0(object):\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def activate(self,inputs):\n",
    "        \"\"\"\n",
    "        Takes in @param inputs, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\" \n",
    "        activation = np.sum(np.dot(inputs, self.weights)) - self.threshold\n",
    "        result = (1 if activation > self.threshold else 0)\n",
    "            \n",
    "        return result\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    p1 = Perceptron0(np.array([1, 2]), 0.)\n",
    "    assert p1.activate(np.array([ 1,-1])) == 0 # < threshold --> 0\n",
    "    assert p1.activate(np.array([-1, 1])) == 1 # > threshold --> 1\n",
    "    assert p1.activate(np.array([ 2,-1])) == 0 # on threshold --> 0\n",
    "    return True\n",
    "\n",
    "if test():\n",
    "    print \"All tests completed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "What are the advantages of using some threshold and step function rather than just outputting the weighted inputs (dot product)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What _parameter_ is learnable in a perceptron? In other words, what can be modified to allow the perceptron or a network of perceptrons to model an arbitrary function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "What does the input to a network of perceptrons look like?\n",
    "A) Tensor of weights\n",
    "B) Matrix of numerical values\n",
    "C) Matrix of classifcations \n",
    "D) Matrix of numerical values and classifications for each row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Are Neural Networks used for classification or regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron update rule\n",
    "\n",
    "Remember the update rule for perceptrons.\n",
    "$$ \\Delta w = (\\eta * (y_i - \\hat{y})) * x_i $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n",
      "[1 2 3]\n",
      "[3 0 2]\n",
      "All tests completed sucessfully\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will update the perceptron class so that it can update\n",
    "# its weights.\n",
    "#\n",
    "# Finish writing the update() method so that it updates the weights according\n",
    "# to the perceptron update rule. Updates should be performed online, revising\n",
    "# the weights after each data point.\n",
    "# \n",
    "# ----------\n",
    "\n",
    "\n",
    "class Perceptron1(Perceptron0):\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        super(Perceptron1,self).__init__(*args )\n",
    "\n",
    "\n",
    "    def update(self, X, y, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param X consisting of a LIST of inputs and a\n",
    "        1D array @param y, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to the perceptron training\n",
    "        rule using these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "        \n",
    "        for setData, yVal in zip(X, y):\n",
    "            yprediction = self.activate(setData)\n",
    "            self.weights = self.weights + np.dot(np.dot(eta, (yVal - yprediction)), setData)\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-6):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    p1 = Perceptron1(np.array([1,1,1]),0)\n",
    "    p1.update(np.array([[2,0,-3]]), np.array([1]))\n",
    "    assert sum_almost_equal(p1.weights, np.array([1.2, 1, 0.7]))\n",
    "    \n",
    "    p2 = Perceptron1(np.array([1,2,3]),0)\n",
    "    p2.update(np.array([[3,2,1],[4,0,-1]]),np.array([0,0]))\n",
    "    assert sum_almost_equal(p2.weights, np.array([0.7, 1.8, 2.9]))\n",
    "    \n",
    "    p3 = Perceptron1(np.array([3,0,2]),0)\n",
    "    p3.update(np.array([[2,-2,4],[-1,-3,2],[0,2,1]]),np.array([0,1,0]))\n",
    "    assert sum_almost_equal(p3.weights, np.array([2.7, -0.3, 1.7]))\n",
    "\n",
    "\n",
    "    return True\n",
    "if test():\n",
    "    print \"All tests completed sucessfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks\n",
    "So far we've just covered single node perceptron units. As you saw in the videos this week, there are limitations to the types of data single percertron units can classifiy--namely they only work on linear serperable data. Let's investigate putting together mutiple layers of perceptron units. Networks of these units work by passing around weights though the nodes of the network until they reach the final output node layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Layered Network\n",
    "Layered networks consist of some input layer, some number hidden nodes, and some output layer that outputs the classifcation or regression results. \n",
    "\n",
    "#### Question\n",
    "Given the network structure below with weights on the edges of the graph what will be the output of this network?\n",
    "\n",
    "![](Network1.png \"NN\")\n",
    "\n",
    "```\n",
    "[\n",
    "[ [1], [2], [3] ]     # Input layer (these are not weights, but input values)\n",
    "[[1,1,-5],[3,-4,2] ]  # Hidden layer\n",
    "[ [2,-1] ]            # Output layer\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build an XOR Network\n",
    "\n",
    "In this exercise you will build a network capable of modeling the exclusive OR funtion. The weights are given below for a threshold of 1.\n",
    "![](XORweights.png \"XOR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-54-68cb1796d015>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-54-68cb1796d015>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    [ ... ], \\\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will create a network of perceptrons that can represent\n",
    "# the XOR function, using a network structure like those shown in the previous\n",
    "# quizzes.\n",
    "#\n",
    "# You will need to do two things:\n",
    "# First, create a network of perceptrons with the correct weights\n",
    "# Second, define a procedure EvalNetwork() which takes in a list of inputs and\n",
    "# outputs the value of this network.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "\n",
    "# Part 1: Set up the perceptron network\n",
    "Network = [\n",
    "    # input layer, declare input layer perceptrons here\n",
    "    [ ... ], \\\n",
    "    # output node, declare output layer perceptron here\n",
    "    [ ... ]\n",
    "]\n",
    "\n",
    "# Part 2: Define a procedure to compute the output of the network, given inputs\n",
    "def EvalNetwork(inputValues, Network):\n",
    "    \"\"\"\n",
    "    Takes in @param inputValues, a list of input values, and @param Network\n",
    "    that specifies a perceptron network. @return the output of the Network for\n",
    "    the given set of inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Be sure your output value is a single number\n",
    "    return OutputValue\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    \"\"\"\n",
    "    print \"0 XOR 0 = 0?:\", EvalNetwork(np.array([0,0]), Network)\n",
    "    print \"0 XOR 1 = 1?:\", EvalNetwork(np.array([0,1]), Network)\n",
    "    print \"1 XOR 0 = 1?:\", EvalNetwork(np.array([1,0]), Network)\n",
    "    print \"1 XOR 1 = 0?:\", EvalNetwork(np.array([1,1]), Network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Discretion\n",
    "\n",
    "The outputs of perceptron units are discrete. Consider a network with the structure [2,2,1], that is 2 input nodes, two hidden nodes, and 1 output node. How many possible outputs to this network are there? _Hint: The answer is NOT two_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coninuity \n",
    "As discussed in the previous lesson, to solve the problem of having only a very few discrete outputs from our neural net, we'll apply a transition function.\n",
    "\n",
    "We'll start by letting you test out a variety of functions, numerically approximating their derivatives in order to apply a gradient descent update rule.\n",
    "\n",
    "**You'll want to run this in the [Udacity sandbox](https://classroom.udacity.com/nanodegrees/nd009/parts/596c7dc6-8049-4785-adfe-7c83ca19b00f/modules/d9424f23-4c4f-452f-8215-c48ae6fc0a56/lessons/7224990821/concepts/72258808312589030923#) to test on their dataset**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# \n",
    "# Python Neural Networks code originally by Szabo Roland and used with\n",
    "# permission\n",
    "#\n",
    "# Modifications, comments, and exercise breakdowns by Mitchell Owen,\n",
    "# (c) Udacity\n",
    "#\n",
    "# Retrieved originally from http://rolisz.ro/2013/04/18/neural-networks-in-python/\n",
    "#\n",
    "#\n",
    "# Neural Network Sandbox\n",
    "#\n",
    "# Define an activation function activate(), which takes in a number and\n",
    "# returns a number.\n",
    "# Using test run you can see the performance of a neural network running with\n",
    "# that activation function, where the inputs are 8x8 images of digits (0-9) and\n",
    "# the outputs are digit predictions made by the network.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "\n",
    "def activate(strength):\n",
    "    # Try out different functions here. Input strength will be a number, with\n",
    "    # another number as output.\n",
    "    return np.power(strength,2)\n",
    "    \n",
    "def activation_derivative(activate, strength):\n",
    "    #numerically approximate\n",
    "    return (activate(strength+1e-5)-activate(strength-1e-5))/(2e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid unit\n",
    "\n",
    "A sigmoid unit is similar to a perceptron unit, but the activation funtion is the non-linear and differentiable logistic function. \n",
    "\n",
    "$$ \\frac{\\mathrm d}{\\mathrm d x} \\left( \\frac{\\mathrm 1}{\\mathrm 1 + e ^ x} \\right) =\\frac{\\mathrm 1}{\\mathrm 1 + e ^ x} (1 - \\frac{\\mathrm 1}{\\mathrm 1 + e ^ x} )  $$\n",
    "\n",
    "$$ \\Delta w = \\eta*(y-\\hat{y}) *( \\frac{\\mathrm d}{\\mathrm d x} S(x_i) )*( x_i ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ee6ecf55656e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msum_almost_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.030739\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.984961\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.027437\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0;32mif\u001b[0m  \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"All tests completed sucessfully\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ee6ecf55656e>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mu1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.880797\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mu1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ee6ecf55656e>\u001b[0m in \u001b[0;36mactivate\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#   logistic function since you will need it for the update function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# ----------\n",
    "# \n",
    "# As with the previous perceptron exercises, you will complete some of the core\n",
    "# methods of a sigmoid unit class.\n",
    "#\n",
    "# There are two functions for you to finish:\n",
    "# First, in activate(), write the sigmoid activation function.\n",
    "# Second, in update(), write the gradient descent update rule. Updates should be\n",
    "#   performed online, revising the weights after each data point.\n",
    "# \n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with sigmoid activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1])):\n",
    "        \"\"\"\n",
    "        Initialize weights based on input arguments. Note that no type-checking\n",
    "        is being performed here for simplicity of code.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "\n",
    "        # NOTE: You do not need to worry about these two attribues for this\n",
    "        # programming quiz, but these will be useful for if you want to create\n",
    "        # a network out of these sigmoid units!\n",
    "        self.last_input = 0 # strength of last input\n",
    "        self.delta      = 0 # error signal\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a sigmoid unit with given inputs based on unit\n",
    "        weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # First calculate the strength of the input signal.\n",
    "        strength = np.dot(values, self.weights)\n",
    "        self.last_input = strength\n",
    "        \n",
    "        # TODO: Modify strength using the sigmoid activation function and\n",
    "        # return as output signal.\n",
    "        # HINT: You may want to create a helper function to compute the\n",
    "        #   logistic function since you will need it for the update function.\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def update(self, values, train, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to gradient descent using\n",
    "        these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: for each data point...\n",
    "        for x_i, y_true in zip(values, train):\n",
    "            # obtain the output signal for that point\n",
    "            y_pred = self.activate(x_i)\n",
    "\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "            # TODO: compute derivative of logistic function at input strength\n",
    "            # Recall: d/dx logistic(x) = logistic(x)*(1-logistic(x))\n",
    "\n",
    "            # TODO: update self.weights based on learning rate, signal accuracy,\n",
    "            # function slope (derivative) and input value\n",
    "            \n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-5):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    u1 = Sigmoid(weights=[3,-2,1])\n",
    "    assert abs(u1.activate(np.array([1,2,3])) - 0.880797) < 1e-5\n",
    "    \n",
    "    u1.update(np.array([[1,2,3]]),np.array([0]))\n",
    "    assert sum_almost_equal(u1.weights, np.array([2.990752, -2.018496, 0.972257]))\n",
    "\n",
    "    u2 = Sigmoid(weights=[0,3,-1])\n",
    "    u2.update(np.array([[-3,-1,2],[2,1,2]]),np.array([1,0]))\n",
    "    assert sum_almost_equal(u2.weights, np.array([-0.030739, 2.984961, -1.027437]))\n",
    "\n",
    "if  test():\n",
    "    print \"All tests completed sucessfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
